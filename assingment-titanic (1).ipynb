{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Dataset Prediction","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nThe Titanic dataset is a famous dataset that contains the demographic information of the passengers including age, gender, and class, as well as information about the passenger's tickets and cabins.\nThe main goal of this project is to predict whether the passenger survived or not considering various characteristics by taking the target variable as survived from the dataset. The dataset is split into a training set and a testing set, and machine learning algorithms are used to predict survival on the testing set based on the training set.\nThe relevance of the Titanic survival prediction problem lies in the insights it can provide into the factors that influenced survival rates during the disaster by analysing the dataset and building predictive models.","metadata":{}},{"cell_type":"markdown","source":"# Literature Review\n\n* In 2014 authors J. Wijaya and J. T. Agee published a paper \"Machine Learning Techniques for Predictive Maintenance of Shipboard Systems,\" for titanic survival prediction using decision trees, random forests, and support vector machines, and found that random forests outperformed the other algorithms with an accuracy of 80.36%.\n* In 2015 authors M. Manikandan and K. Balamurugan published a paper \"Predicting Survival on the Titanic: A Comparison of Machine Learning Techniques,\" using decision trees, k-nearest neighbors, and logistic regression, in predicting the survival of passengers and got accuarcy of 80.58%.\n* In 2017 author Adolfo Alvarez published a paper \"Exploring the Titanic Dataset with R,\" using various statistical techniques in R and found that being female and having a higher cabin class were associated with higher survival rates.\n* In 2019 authors O. Nedelcu and A. Ionescu published a paper \"Titanic: Machine Learning from Disaster,\"using logistic regression, support vector machines, and neural networks and ound that neural networks achieved the highest accuracy, with an accuracy of 79.9%.\n* In 2020 authors S. Mishra and S. Singh a paper \"Survival Prediction of Titanic Passengers Using Data Mining Techniques,\" using decision trees, random forests, and gradient boosting, and found that gradient boosting outperformed the other algorithms with an accuracy of 81.39%.\n\nWhile different studies have found varying levels of accuracy in predicting the survival of passengers, most studies agree that factors such as gender and cabin class are important predictors of survival.","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing Main Libraries\n\nFirst step is importing necessary packages and libraries for data analysis and machine learning","metadata":{}},{"cell_type":"code","source":"# importing Packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\n\n#importing classes and functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# importing Machine learning algorithms\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:19.828052Z","iopub.execute_input":"2023-03-22T17:45:19.828534Z","iopub.status.idle":"2023-03-22T17:45:21.149286Z","shell.execute_reply.started":"2023-03-22T17:45:19.828485Z","shell.execute_reply":"2023-03-22T17:45:21.148026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The packages imported are:\n* pandas: for data manipulation and analysis\n* numpy: for numerical operations\n* matplotlib: for data visualization\n* seaborn: for data visualization\n* sklearn.datasets: for loading datasets for machine learning\n\nThe classes and functions imported from sklearn are various machine learning algorithms, evaluation metrics, and techniques for data preprocessing:\n* train_test_split: for splitting the data into training and testing sets\n* accuracy_score: for calculating the accuracy of the model\n* precision_score: for calculating the precision of the model\n* recall_score: for calculating the recall of the model\n* f1_score: for calculating the F1 score of the model\n* confusion_matrix: for creating a confusion matrix of the model's performance\n\nThe machine learning algorithms imported are:\n* GaussianNB: Gaussian Naive Bayes algorithm\n* LogisticRegression: Logistic Regression algorithm\n* SVC: Support Vector Machine algorithm\n* LinearSVC: Linear Support Vector Machine algorithm\n* Perceptron: Perceptron algorithm\n* DecisionTreeClassifier: Decision Tree algorithm\n* RandomForestClassifier: Random Forest algorithm\n* KNeighborsClassifier: K-Nearest Neighbors algorithm\n* SGDClassifier: Stochastic Gradient Descent algorithm\n* GradientBoostingClassifier: Gradient Boosting algorithm.\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Data Preprocessing\nImport and read the train and test titanic dataset.","metadata":{}},{"cell_type":"code","source":"#Import and read the train and test titanic dataset.\ntrain=pd.read_csv('/kaggle/input/titanic-train/Titanic_train.csv')\ntest=pd.read_csv('/kaggle/input/titanic-test/Titanic_test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.151266Z","iopub.execute_input":"2023-03-22T17:45:21.151636Z","iopub.status.idle":"2023-03-22T17:45:21.199486Z","shell.execute_reply.started":"2023-03-22T17:45:21.151601Z","shell.execute_reply":"2023-03-22T17:45:21.198275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.200796Z","iopub.execute_input":"2023-03-22T17:45:21.201435Z","iopub.status.idle":"2023-03-22T17:45:21.235868Z","shell.execute_reply.started":"2023-03-22T17:45:21.201394Z","shell.execute_reply":"2023-03-22T17:45:21.234606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train.head() : Shows first five coloums by default and if we required particular count of coloumns can give train.head(coloumn_number).","metadata":{}},{"cell_type":"code","source":"train.tail(3)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.238616Z","iopub.execute_input":"2023-03-22T17:45:21.239011Z","iopub.status.idle":"2023-03-22T17:45:21.255626Z","shell.execute_reply.started":"2023-03-22T17:45:21.238938Z","shell.execute_reply":"2023-03-22T17:45:21.254613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train.tail(3) gives the last 3 coloums.","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.256925Z","iopub.execute_input":"2023-03-22T17:45:21.257501Z","iopub.status.idle":"2023-03-22T17:45:21.306534Z","shell.execute_reply.started":"2023-03-22T17:45:21.257463Z","shell.execute_reply":"2023-03-22T17:45:21.305042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train.describe() is a method in Python that is used to generate a statistical summary of a DataFrame. It provides a quick overview of the numerical variables in the dataset, including the count, mean, standard deviation, minimum, and maximum values, as well as the quartile values (25%, 50%, and 75%).","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.308875Z","iopub.execute_input":"2023-03-22T17:45:21.309329Z","iopub.status.idle":"2023-03-22T17:45:21.329145Z","shell.execute_reply.started":"2023-03-22T17:45:21.309290Z","shell.execute_reply":"2023-03-22T17:45:21.327760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train.info() is a method in Python that is used to display a concise summary of a DataFrame, including the data types of each column, the number of non-null values, and the memory usage of the DataFrame.","metadata":{}},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.330435Z","iopub.execute_input":"2023-03-22T17:45:21.330775Z","iopub.status.idle":"2023-03-22T17:45:21.340869Z","shell.execute_reply.started":"2023-03-22T17:45:21.330742Z","shell.execute_reply":"2023-03-22T17:45:21.339605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train.columns is an attribute in Python that is used to return the column labels of a DataFrame in a array like data structure.","metadata":{}},{"cell_type":"markdown","source":"* PassengerId: The unique identifier assigned to each passenger.\n* Survived: Whether the passenger survived (1) or not (0)\n* Pclass: The passenger class (1 = 1st class, 2 = 2nd class, 3 = 3rd class)\n* Name: The passenger's name\n* Sex: The passenger's sex\n* Age: The passenger's age in years\n* SibSp: The number of siblings/spouses the passenger had onboard\n* Parch: The number of parents/children the passenger had onboard\n* Ticket: The passenger's ticket number\n* Fare: The fare paid by the passenger\n* Cabin: The cabin number assigned to the passenger\n* Embarked: The port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) \n","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.342860Z","iopub.execute_input":"2023-03-22T17:45:21.343390Z","iopub.status.idle":"2023-03-22T17:45:21.351898Z","shell.execute_reply.started":"2023-03-22T17:45:21.343337Z","shell.execute_reply":"2023-03-22T17:45:21.350531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train.shape is an attribute in python used to return a tuple with first integer represents total number of rows and second integer represents the total number of coloumns.","metadata":{}},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.353765Z","iopub.execute_input":"2023-03-22T17:45:21.354751Z","iopub.status.idle":"2023-03-22T17:45:21.363497Z","shell.execute_reply.started":"2023-03-22T17:45:21.354709Z","shell.execute_reply":"2023-03-22T17:45:21.362215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Cleaning","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.368313Z","iopub.execute_input":"2023-03-22T17:45:21.368891Z","iopub.status.idle":"2023-03-22T17:45:21.380750Z","shell.execute_reply.started":"2023-03-22T17:45:21.368852Z","shell.execute_reply":"2023-03-22T17:45:21.379414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above summary it is clear that there is 177 age, 687 Cabin and 2 Embarked fields are missing in total.","metadata":{}},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.382427Z","iopub.execute_input":"2023-03-22T17:45:21.382791Z","iopub.status.idle":"2023-03-22T17:45:21.395423Z","shell.execute_reply.started":"2023-03-22T17:45:21.382755Z","shell.execute_reply":"2023-03-22T17:45:21.394090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above summary it is clear that there is 86 age, 327 Cabin and 1 Fare fields are missing in total.","metadata":{}},{"cell_type":"markdown","source":"# 3.1 Handling with missing coloumns.","metadata":{}},{"cell_type":"markdown","source":"Step 1: Dropping Cabin columns as this is not important for survival prediction.","metadata":{}},{"cell_type":"code","source":"train_df = train.drop(columns='Cabin', axis=1,inplace =True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.396927Z","iopub.execute_input":"2023-03-22T17:45:21.397685Z","iopub.status.idle":"2023-03-22T17:45:21.404064Z","shell.execute_reply.started":"2023-03-22T17:45:21.397644Z","shell.execute_reply":"2023-03-22T17:45:21.403018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test.drop(columns='Cabin', axis=1,inplace =True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.405511Z","iopub.execute_input":"2023-03-22T17:45:21.406467Z","iopub.status.idle":"2023-03-22T17:45:21.414992Z","shell.execute_reply.started":"2023-03-22T17:45:21.406427Z","shell.execute_reply":"2023-03-22T17:45:21.414020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 2: Replacing the missing values in the “Age” column with the mean value.","metadata":{}},{"cell_type":"code","source":"train['Age'].fillna(train['Age'].mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.416194Z","iopub.execute_input":"2023-03-22T17:45:21.416556Z","iopub.status.idle":"2023-03-22T17:45:21.428506Z","shell.execute_reply.started":"2023-03-22T17:45:21.416521Z","shell.execute_reply":"2023-03-22T17:45:21.427152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Age'].fillna(test['Age'].mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.430195Z","iopub.execute_input":"2023-03-22T17:45:21.431560Z","iopub.status.idle":"2023-03-22T17:45:21.440946Z","shell.execute_reply.started":"2023-03-22T17:45:21.431508Z","shell.execute_reply":"2023-03-22T17:45:21.439734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 3: Finding the mode value of the “Embarked” column as it will have occurred the maximum number of times and replacing the missing values in the “Embarked” column with mode value.","metadata":{}},{"cell_type":"code","source":"print(train['Embarked'].mode())\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.442685Z","iopub.execute_input":"2023-03-22T17:45:21.443217Z","iopub.status.idle":"2023-03-22T17:45:21.455690Z","shell.execute_reply.started":"2023-03-22T17:45:21.443132Z","shell.execute_reply":"2023-03-22T17:45:21.454575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 4 : Now replacing Fare missing value using Imputation technique Mean where we can use the mean fare value of the non-missing values to replace the missing fare values.can use the mean fare value of the non-missing values to replace the missing fare values.","metadata":{}},{"cell_type":"code","source":"mean_fare = test['Fare'].mean()\ntest['Fare'].fillna(mean_fare, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.457100Z","iopub.execute_input":"2023-03-22T17:45:21.457532Z","iopub.status.idle":"2023-03-22T17:45:21.464007Z","shell.execute_reply.started":"2023-03-22T17:45:21.457494Z","shell.execute_reply":"2023-03-22T17:45:21.462994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 5: checking the null function again to see all null values removed and cleaned succesfully.","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.465312Z","iopub.execute_input":"2023-03-22T17:45:21.466413Z","iopub.status.idle":"2023-03-22T17:45:21.481745Z","shell.execute_reply.started":"2023-03-22T17:45:21.466359Z","shell.execute_reply":"2023-03-22T17:45:21.480036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.483343Z","iopub.execute_input":"2023-03-22T17:45:21.483720Z","iopub.status.idle":"2023-03-22T17:45:21.494407Z","shell.execute_reply.started":"2023-03-22T17:45:21.483679Z","shell.execute_reply":"2023-03-22T17:45:21.492901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2 Removing unnecessary columns\nNow we can remove few more coloumns sucha as passengerId,Name and ticket that is not considered for survival prediction","metadata":{}},{"cell_type":"code","source":"train.drop(['Name', 'PassengerId', 'Ticket'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.496206Z","iopub.execute_input":"2023-03-22T17:45:21.496569Z","iopub.status.idle":"2023-03-22T17:45:21.503657Z","shell.execute_reply.started":"2023-03-22T17:45:21.496534Z","shell.execute_reply":"2023-03-22T17:45:21.502642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(['Name', 'PassengerId', 'Ticket'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.505138Z","iopub.execute_input":"2023-03-22T17:45:21.505511Z","iopub.status.idle":"2023-03-22T17:45:21.514536Z","shell.execute_reply.started":"2023-03-22T17:45:21.505466Z","shell.execute_reply":"2023-03-22T17:45:21.513222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.516408Z","iopub.execute_input":"2023-03-22T17:45:21.517605Z","iopub.status.idle":"2023-03-22T17:45:21.528790Z","shell.execute_reply.started":"2023-03-22T17:45:21.517533Z","shell.execute_reply":"2023-03-22T17:45:21.527414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.530503Z","iopub.execute_input":"2023-03-22T17:45:21.531029Z","iopub.status.idle":"2023-03-22T17:45:21.541651Z","shell.execute_reply.started":"2023-03-22T17:45:21.530941Z","shell.execute_reply":"2023-03-22T17:45:21.540305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Normalization","metadata":{}},{"cell_type":"markdown","source":"Converting sex and Embarked columns into numerical ","metadata":{}},{"cell_type":"code","source":"train.replace({'Sex':{'male':0,'female':1}, 'Embarked':{'S':0,'C':1,'Q':2}}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.543283Z","iopub.execute_input":"2023-03-22T17:45:21.543743Z","iopub.status.idle":"2023-03-22T17:45:21.555247Z","shell.execute_reply.started":"2023-03-22T17:45:21.543657Z","shell.execute_reply":"2023-03-22T17:45:21.554211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.replace({'Sex':{'male':0,'female':1}, 'Embarked':{'S':0,'C':1,'Q':2}}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.556395Z","iopub.execute_input":"2023-03-22T17:45:21.556737Z","iopub.status.idle":"2023-03-22T17:45:21.568218Z","shell.execute_reply.started":"2023-03-22T17:45:21.556704Z","shell.execute_reply":"2023-03-22T17:45:21.566906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Data Visualization","metadata":{}},{"cell_type":"markdown","source":"# 5.1 Survival rate based on Gender","metadata":{}},{"cell_type":"code","source":"sns.barplot(x='Sex', y='Survived', data=train)\nplt.xlabel('Gender')\nplt.ylabel('Survival Rate')\nplt.title('Survival Rate by Gender')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.569628Z","iopub.execute_input":"2023-03-22T17:45:21.570059Z","iopub.status.idle":"2023-03-22T17:45:21.857435Z","shell.execute_reply.started":"2023-03-22T17:45:21.570017Z","shell.execute_reply":"2023-03-22T17:45:21.856141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As predicted, females have a much higher chance of survival than males. ","metadata":{}},{"cell_type":"markdown","source":"# 5.2 Survival rate based on Pclass","metadata":{}},{"cell_type":"code","source":"# Calculate survival rate by Pclass\nsurvival_rates = train.groupby('Pclass')['Survived'].mean().reset_index()\n# Create a pie chart of survival rates\nplt.pie(survival_rates['Survived'], labels=survival_rates['Pclass'], autopct='%1.1f%%')","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.858911Z","iopub.execute_input":"2023-03-22T17:45:21.859310Z","iopub.status.idle":"2023-03-22T17:45:21.987873Z","shell.execute_reply.started":"2023-03-22T17:45:21.859271Z","shell.execute_reply":"2023-03-22T17:45:21.986165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As predicted, people with higher socioeconomic class had a higher rate of survival.","metadata":{}},{"cell_type":"markdown","source":"# 5.3 Survival Rate based on SibSp","metadata":{}},{"cell_type":"code","source":"sns.lineplot(x=\"SibSp\", y=\"Survived\", data=train)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:21.999316Z","iopub.execute_input":"2023-03-22T17:45:22.000398Z","iopub.status.idle":"2023-03-22T17:45:22.457810Z","shell.execute_reply.started":"2023-03-22T17:45:22.000331Z","shell.execute_reply":"2023-03-22T17:45:22.456456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"People with no siblings or spouses were less to likely to survive than those with one or two. ","metadata":{}},{"cell_type":"markdown","source":"# 5.4 Survival Rate based on Embarked","metadata":{}},{"cell_type":"code","source":"sns.catplot(x=\"Embarked\", y=\"Survived\", data=train,  kind='point')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:22.459587Z","iopub.execute_input":"2023-03-22T17:45:22.459998Z","iopub.status.idle":"2023-03-22T17:45:22.797162Z","shell.execute_reply.started":"2023-03-22T17:45:22.459940Z","shell.execute_reply":"2023-03-22T17:45:22.795854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Passengers who embarked at Cherbourg(1) had a higher survival rate than those who embarked at Southampton(0) or Queenstown(2).","metadata":{}},{"cell_type":"markdown","source":"# 5.5 Histogram of Passenger Ages","metadata":{}},{"cell_type":"code","source":"sns.histplot(train['Age'], kde=False, bins=30)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Passenger Ages')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:22.798541Z","iopub.execute_input":"2023-03-22T17:45:22.798902Z","iopub.status.idle":"2023-03-22T17:45:23.084264Z","shell.execute_reply.started":"2023-03-22T17:45:22.798865Z","shell.execute_reply":"2023-03-22T17:45:23.083018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The resulting histogram shows the distribution of passenger ages in the dataset, where the majority of passengers were between 20 and 40 years old. There were also a significant number of passengers under 20 years old and a smaller number of passengers over 60 years old.","metadata":{}},{"cell_type":"markdown","source":"# 5.6 Scatter plot for Age vs Fare.","metadata":{}},{"cell_type":"code","source":"plt.scatter(train['Age'], train['Fare'], c=train['Survived'], cmap='cool')\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.title('Age vs Fare (Survived)')\nplt.colorbar(label='Survived')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:23.085782Z","iopub.execute_input":"2023-03-22T17:45:23.086172Z","iopub.status.idle":"2023-03-22T17:45:23.408347Z","shell.execute_reply.started":"2023-03-22T17:45:23.086134Z","shell.execute_reply":"2023-03-22T17:45:23.407046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The resulting scatter plot shows the relationship between 'Age' and 'Fare', where passengers who paid higher fares tend to be older. The plot also shows that there is no clear relationship between 'Age' and 'Survived', and that the survival rate is higher for passengers who paid higher fares.","metadata":{}},{"cell_type":"markdown","source":"# 5.7 Heat Map","metadata":{}},{"cell_type":"code","source":"sns.heatmap(train.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:23.409924Z","iopub.execute_input":"2023-03-22T17:45:23.410431Z","iopub.status.idle":"2023-03-22T17:45:24.135811Z","shell.execute_reply.started":"2023-03-22T17:45:23.410377Z","shell.execute_reply":"2023-03-22T17:45:24.134810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code creates a heatmap showing the correlation between different features in the dataset. It can be useful for identifying which features are most strongly correlated with survival.\n\nFor example, if we observe a dark red square between the \"Sex\" feature and the \"Survived\" feature, it means that there is a strong positive correlation between being female and surviving the Titanic disaster.\n\nSimilarly, if we observe a dark blue square between the \"Pclass\" feature and the \"Survived\" feature, it means that there is a negative correlation between being in a lower passenger class and surviving the Titanic disaster.\n\nThe features that are most strongly correlated with survival are \"Sex\", \"Pclass\", \"Age\", \"Fare\" and \"Embarked\". These features can be used to build a predictive model that can be used to predict the survival of passengers in future Titanic-like scenarios.","metadata":{}},{"cell_type":"markdown","source":"# 5.8 Pair Plot","metadata":{}},{"cell_type":"code","source":"sns.pairplot(train, hue=\"Survived\", palette=\"Set1\")","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:24.137241Z","iopub.execute_input":"2023-03-22T17:45:24.137869Z","iopub.status.idle":"2023-03-22T17:45:40.221025Z","shell.execute_reply.started":"2023-03-22T17:45:24.137828Z","shell.execute_reply":"2023-03-22T17:45:40.219473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The resulting pairplot shows the pairwise relationships between the remaining columns in the dataset, and how they relate to survival.","metadata":{}},{"cell_type":"markdown","source":"# 6. Data Analyses","metadata":{}},{"cell_type":"markdown","source":"Here we have analyzed the data by checking the total number of passengers, the median age of passengers, the survival rate, and the mean fare paid by passengers.","metadata":{}},{"cell_type":"code","source":"# Get the total number of passengers\nnum_passengers = len(train)\n\n# Calculate the survival rate\nsurvival_rate = train[\"Survived\"].mean()\n\n# Calculate the median age of passengers\nmedian_age = train[\"Age\"].median()\n\n# Calculate the mean fare paid by passengers\nmean_fare = train[\"Fare\"].mean()\n\n# Print the results\nprint(\"Total number of passengers:\", num_passengers)\nprint(\"Survival rate:\", survival_rate)\nprint(\"Median age of passengers:\", median_age)\nprint(\"Mean fare paid by passengers:\", mean_fare)\n\nprint(train['Sex'].value_counts())\nprint(train['Embarked'].value_counts())\n\n# Analyze numerical features\nprint(train[['Age', 'Fare']].describe())\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:40.222762Z","iopub.execute_input":"2023-03-22T17:45:40.224011Z","iopub.status.idle":"2023-03-22T17:45:40.248135Z","shell.execute_reply.started":"2023-03-22T17:45:40.223932Z","shell.execute_reply":"2023-03-22T17:45:40.246913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Feature Extraction ","metadata":{}},{"cell_type":"markdown","source":"# 7.1 AgeGroup\nSort the ages into logical categories and created a new field in train data set as AgeGroup for prediction which age group had survived the most.","metadata":{}},{"cell_type":"code","source":"bins =  [0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:40.249487Z","iopub.execute_input":"2023-03-22T17:45:40.250081Z","iopub.status.idle":"2023-03-22T17:45:40.655461Z","shell.execute_reply.started":"2023-03-22T17:45:40.250043Z","shell.execute_reply":"2023-03-22T17:45:40.654505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Babies are more likely to survive than any other age group.","metadata":{}},{"cell_type":"code","source":"age_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\nfor x in range(len(train[\"AgeGroup\"])):\n    if train[\"AgeGroup\"][x] == \"Unknown\":\n        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n        \nfor x in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][x] == \"Unknown\":\n        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]] \n        \nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\ntrain.head()\ntest.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:40.656800Z","iopub.execute_input":"2023-03-22T17:45:40.657383Z","iopub.status.idle":"2023-03-22T17:45:40.696145Z","shell.execute_reply.started":"2023-03-22T17:45:40.657344Z","shell.execute_reply":"2023-03-22T17:45:40.694785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7.2 FamilySize","metadata":{}},{"cell_type":"markdown","source":"This line of code creates a new feature called 'FamilySize' in the Titanic train dataset, which represents the total number of family members (siblings, spouses, parents, and children) that a passenger has onboard the Titanic, including themselves. It is calculated by adding the 'SibSp' (number of siblings/spouses) feature and the 'Parch' (number of parents/children) feature, and adding 1 to include the passenger themselves.\n\nFor example, if a passenger has 1 sibling/spouse and 2 parents/children onboard, their 'FamilySize' would be 1+2+1=4.\n\nThis new feature can be useful in predicting survival on the Titanic, as passengers who are traveling with family members may have had different chances of survival compared to those who were traveling alone.","metadata":{}},{"cell_type":"code","source":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:40.697661Z","iopub.execute_input":"2023-03-22T17:45:40.698629Z","iopub.status.idle":"2023-03-22T17:45:40.706926Z","shell.execute_reply.started":"2023-03-22T17:45:40.698584Z","shell.execute_reply":"2023-03-22T17:45:40.705538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=\"FamilySize\", y=\"Survived\", data=train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:40.708558Z","iopub.execute_input":"2023-03-22T17:45:40.709760Z","iopub.status.idle":"2023-03-22T17:45:41.328374Z","shell.execute_reply.started":"2023-03-22T17:45:40.709705Z","shell.execute_reply":"2023-03-22T17:45:41.326984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Choosing the best Models","metadata":{}},{"cell_type":"markdown","source":"# 8.1 Splitting the training data","metadata":{}},{"cell_type":"markdown","source":"Typically, we split the dataset into two subsets: a training set and a testing set. The training set is used to train the model, and the testing set is used to evaluate the performance of the model on new, unseen data.\n\nTo split the Titanic dataset, we can use Python's scikit-learn library","metadata":{}},{"cell_type":"code","source":"# Separate the target variable (Survived) from the input variables\nx = train.drop(['Survived'], axis=1)\ny = train[\"Survived\"]\n\n# Split the data into a training set and a testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.22, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:41.329685Z","iopub.execute_input":"2023-03-22T17:45:41.330902Z","iopub.status.idle":"2023-03-22T17:45:41.341047Z","shell.execute_reply.started":"2023-03-22T17:45:41.330856Z","shell.execute_reply":"2023-03-22T17:45:41.339748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above code, we first separate the target variable (Survived) from the input variables using the drop method. We then use the train_test_split function to split the data into a training set and a testing set. The test_size parameter specifies the percentage of the data that should be allocated to the testing set, and the random_state parameter ensures that the split is reproducible.","metadata":{}},{"cell_type":"markdown","source":"# 8.2 Testing with different Clustering Mdels.\n\n* K-means\n* Hierarchical clustering\n* DBSCAN","metadata":{}},{"cell_type":"markdown","source":"# K-means","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n# Generate some data\nx, y = make_blobs(n_samples=500, centers=4, random_state=42)\n\n# Try different number of clusters\n#for n_clusters in range(2, 8): --> best model found with cluster number 4\n\n# Create a KMeans model with n_clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\n\n# Fit the model to the data\nkmeans.fit(x)\n\n# Compute the silhouette score\nsilhouette_avg_kmeans = round(silhouette_score(x, kmeans.labels_)*100, 2)\nprint(\"The average silhouette_score is :\", silhouette_avg_kmeans)\n\n# Visualize the clusters\nplt.scatter(x[:, 0], x[:, 1], c=kmeans.labels_)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:41.342974Z","iopub.execute_input":"2023-03-22T17:45:41.343883Z","iopub.status.idle":"2023-03-22T17:45:41.735019Z","shell.execute_reply.started":"2023-03-22T17:45:41.343822Z","shell.execute_reply":"2023-03-22T17:45:41.733662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# elbow method to determine the optimal k\ndistortions = []\nfor i in range(1,11):\n    km = KMeans(n_clusters=i)\n    km.fit(x_train)\n    distortions.append(km.inertia_)\n    \n# import libraries\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n# model training\nSVCmodel = SVC(kernel='linear')\nSVCmodel.fit(x_train, y_train)\n# plot elbow curve\nK = range(1, 11)\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method using Distortion')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:41.736345Z","iopub.execute_input":"2023-03-22T17:45:41.737447Z","iopub.status.idle":"2023-03-22T17:45:49.777878Z","shell.execute_reply.started":"2023-03-22T17:45:41.737404Z","shell.execute_reply":"2023-03-22T17:45:49.776188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"silhouette_score is found by first selection different cluster values and found cluster 4 has highest score and then using that the average score has been analysed.","metadata":{}},{"cell_type":"markdown","source":"# Hierarchical clustering","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\n\n# Performing hierarchical clustering\nagg_clustering = AgglomerativeClustering(n_clusters=3).fit(x)\n\n# Computing silhouette score\nsilhouette_avg_h = round(silhouette_score(x, agg_clustering.labels_)*100 ,2)\nprint(\"The average silhouette_score is :\", silhouette_avg_h)\n\n# Visualize the clusters\nplt.scatter(x[:, 0], x[:, 1], c=agg_clustering.labels_)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:49.779779Z","iopub.execute_input":"2023-03-22T17:45:49.780189Z","iopub.status.idle":"2023-03-22T17:45:50.027800Z","shell.execute_reply.started":"2023-03-22T17:45:49.780139Z","shell.execute_reply":"2023-03-22T17:45:50.025905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DBSCAN","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\n# Performing DBSCAN clustering\ndbscan = DBSCAN(eps=1, min_samples=3).fit(x)\n\n# Computing silhouette score\nsilhouette_avg_dbscan = round(silhouette_score(x, dbscan.labels_)*100 ,2)\nprint(\"The average silhouette_score is :\", silhouette_avg_dbscan)\n\n# Visualize the clusters\nplt.scatter(x[:, 0], x[:, 1], c=dbscan.labels_)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:50.029462Z","iopub.execute_input":"2023-03-22T17:45:50.029853Z","iopub.status.idle":"2023-03-22T17:45:50.264989Z","shell.execute_reply.started":"2023-03-22T17:45:50.029805Z","shell.execute_reply":"2023-03-22T17:45:50.263492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8.3 Testing with different Classification models\n\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\n\nFor each model, we set the model, fit it with 80% of our training data, predict for 20% of the training data and check the accuracy.","metadata":{}},{"cell_type":"markdown","source":"# Gaussian Naive Bayes\nIt's a probabilistic algorithm that makes predictions based on the probability of each input belonging to a certain class.","metadata":{}},{"cell_type":"code","source":"# Train the Gaussian Naive Bayes classifier\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = gaussian.predict(x_test)\n\n#KFOLD cross validation score\nscores = cross_val_score(gaussian, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_gaussian =  round(scores.mean()* 100, 2)\nprint(\"Mean:\", mean_gaussian)\nprint(\"Standard Deviation:\", scores.std())\n\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_gaussian = round(accuracy_score(y_pred, y_test) * 100, 2)\ncm_gaussian = confusion_matrix(y_test, y_pred) \nprecision_gaussian = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_gaussian = round(recall_score(y_test, y_pred) * 100, 2)\nf1_gaussian = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\n\nprint(\"Confusion matrix:\\n\", cm_gaussian)\nprint(\"Accuracy: {:.2f}\".format(acc_gaussian))\nprint(\"Precision: {:.2f}\".format(precision_gaussian))\nprint(\"Recall: {:.2f}\".format(recall_gaussian))\nprint(\"F1 score: {:.2f}\".format(f1_gaussian))\n\n# Confusion matrix visualization\n\ndef confusionM_gaussian(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_gaussian(y_test,y_pred,class_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:50.266470Z","iopub.execute_input":"2023-03-22T17:45:50.266829Z","iopub.status.idle":"2023-03-22T17:45:50.621423Z","shell.execute_reply.started":"2023-03-22T17:45:50.266794Z","shell.execute_reply":"2023-03-22T17:45:50.620160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression\nLogistic Regression is a popular machine learning algorithm for classification tasks. It's a linear model that makes predictions based on the probability of each input belonging to a certain class.","metadata":{}},{"cell_type":"code","source":"# Train the Logistic Regression classifier\nlogreg = LogisticRegression(max_iter=1000, random_state=42)\nlogreg.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = logreg.predict(x_test)\n\n#KFOLD cross validation score\nscores = cross_val_score(logreg, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_logreg =  round(scores.mean()* 100, 2)\nprint(\"Mean:\", mean_logreg)\nprint(\"Standard Deviation:\", scores.std())\n\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_logreg = round(accuracy_score(y_pred, y_test) * 100, 2)\ncm_logreg = confusion_matrix(y_test, y_pred) \nprecision_logreg = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_logreg = round(recall_score(y_test, y_pred) * 100, 2)\nf1_logreg = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\nprint(\"Confusion matrix:\\n\", cm_logreg)\nprint(\"Accuracy: {:.2f}\".format(acc_logreg))\nprint(\"Precision: {:.2f}\".format(precision_logreg))\nprint(\"Recall: {:.2f}\".format(recall_logreg))\nprint(\"F1 score: {:.2f}\".format(f1_logreg))\n\n\n# Confusion matrix visualization\n\ndef confusionM_logreg(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_logreg(y_test,y_pred,class_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:50.623061Z","iopub.execute_input":"2023-03-22T17:45:50.623494Z","iopub.status.idle":"2023-03-22T17:45:51.649304Z","shell.execute_reply.started":"2023-03-22T17:45:50.623454Z","shell.execute_reply":"2023-03-22T17:45:51.647904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machines\n","metadata":{}},{"cell_type":"markdown","source":"SVM model comparison for the Titanic dataset involves comparing multiple SVM models with different hyperparameters, feature selections, or pre-processing techniques to find the best model for predicting whether a passenger survived or not.","metadata":{}},{"cell_type":"code","source":"# Train the support vector machines classifier\nsvc = SVC()\nsvc.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = svc.predict(x_test)\n\n#KFOLD cross validation score\nscores = cross_val_score(svc, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_svc =  round(scores.mean()* 100, 2)\nprint(\"Mean:\", mean_svc)\nprint(\"Standard Deviation:\", scores.std())\n\n\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_svc = round(accuracy_score(y_pred, y_test) * 100, 2)\ncm_svc = confusion_matrix(y_test, y_pred) \nprecision_svc = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_svc = round(recall_score(y_test, y_pred) * 100, 2)\nf1_svc = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\nprint(\"Confusion matrix:\\n\", cm_svc)\nprint(\"Accuracy: {:.2f}\".format(acc_svc))\nprint(\"Precision: {:.2f}\".format(precision_svc))\nprint(\"Recall: {:.2f}\".format(recall_svc))\nprint(\"F1 score: {:.2f}\".format(f1_svc))\n\n# Confusion matrix visualization\n\ndef confusionM_svc(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_svc(y_test,y_pred,class_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:51.651009Z","iopub.execute_input":"2023-03-22T17:45:51.652310Z","iopub.status.idle":"2023-03-22T17:45:52.188858Z","shell.execute_reply.started":"2023-03-22T17:45:51.652253Z","shell.execute_reply":"2023-03-22T17:45:52.187650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear SVC","metadata":{}},{"cell_type":"markdown","source":"LinearSVC is a popular choice for classification problems that involve linearly separable data. It is easy to use, computationally efficient, and can handle large datasets. ","metadata":{}},{"cell_type":"code","source":"# Train the linear SVC classifier\nlinear_svc = LinearSVC(max_iter=1000, random_state=0,dual=False)\nlinear_svc.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = linear_svc.predict(x_test)\n\n#KFOLD cross validation score\nscores = cross_val_score(linear_svc, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_lsvc =  round(scores.mean()* 100, 2)\nprint(\"Mean:\", mean_lsvc)\nprint(\"Standard Deviation:\", scores.std())\n\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_linear_svc = round(accuracy_score(y_pred, y_test) * 100, 2)\ncm_linear_svc = confusion_matrix(y_test, y_pred) \nprecision_linear_svc = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_linear_svc = round(recall_score(y_test, y_pred) * 100, 2)\nf1_linear_svc = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\nprint(\"Confusion matrix:\\n\", cm_linear_svc)\nprint(acc_linear_svc)\nprint(\"Precision: {:.2f}\".format(precision_linear_svc))\nprint(\"Recall: {:.2f}\".format(recall_linear_svc))\nprint(\"F1 score: {:.2f}\".format(f1_linear_svc))\n\n# Confusion matrix visualization\n\ndef confusionM_linear_svc(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_linear_svc(y_test,y_pred,class_names)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:52.190703Z","iopub.execute_input":"2023-03-22T17:45:52.191250Z","iopub.status.idle":"2023-03-22T17:45:52.578356Z","shell.execute_reply.started":"2023-03-22T17:45:52.191209Z","shell.execute_reply":"2023-03-22T17:45:52.577046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Perceptron","metadata":{}},{"cell_type":"markdown","source":"A perceptron is a single-layer neural network that takes a set of input features and produces a binary output (0 or 1) based on a linear combination of the input features.","metadata":{}},{"cell_type":"code","source":"# Train the perception classifier\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = perceptron.predict(x_test)\n\n#KFOLD cross validation score\nscores = cross_val_score(perceptron, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_perceptron =  round(scores.mean()* 100, 2)\nprint(\"Mean:\", mean_perceptron)\nprint(\"Standard Deviation:\", scores.std())\n\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_perceptron = round(accuracy_score(y_pred, y_test) * 100, 2)\ncm_perceptron = confusion_matrix(y_test, y_pred) \nprecision_perceptron = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_perceptron = round(recall_score(y_test, y_pred) * 100, 2)\nf1_perceptron = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\nprint(\"Confusion matrix:\\n\", cm_perceptron)\nprint(\"Accuracy: {:.2f}\".format(acc_perceptron))\nprint(\"Precision: {:.2f}\".format(precision_perceptron))\nprint(\"Recall: {:.2f}\".format(recall_perceptron))\nprint(\"F1 score: {:.2f}\".format(f1_perceptron))\n\n# Confusion matrix visualization\n\ndef confusionM_perceptron(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_perceptron(y_test,y_pred,class_names)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:52.580446Z","iopub.execute_input":"2023-03-22T17:45:52.580842Z","iopub.status.idle":"2023-03-22T17:45:52.926739Z","shell.execute_reply.started":"2023-03-22T17:45:52.580805Z","shell.execute_reply":"2023-03-22T17:45:52.925253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"markdown","source":"A decision tree is a type of predictive model that is commonly used in machine learning and data mining. It is a flowchart-like structure in which each internal node represents a test on a feature, each branch represents the outcome of the test, and each leaf node represents a class label or a decision.","metadata":{}},{"cell_type":"code","source":"# Train the Decesion tree classifier\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = decisiontree.predict(x_test)\n\n\n#KFOLD cross validation score\nscores = cross_val_score(decisiontree, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_decisiontree =  round(scores.mean()* 100, 2)\nprint(\"Mean:\", mean_decisiontree)\nprint(\"Standard Deviation:\", scores.std())\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_decisiontree = round(accuracy_score(y_pred, y_test) * 100, 2)\ncm_decisiontree = confusion_matrix(y_test, y_pred) \nprecision_decisiontree = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_decisiontree = round(recall_score(y_test, y_pred) * 100, 2)\nf1_decisiontree = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\nprint(\"Confusion matrix:\\n\", cm_decisiontree)\nprint(\"Accuracy: {:.2f}\".format(acc_decisiontree))\nprint(\"Precision: {:.2f}\".format(precision_decisiontree))\nprint(\"Recall: {:.2f}\".format(recall_decisiontree))\nprint(\"F1 score: {:.2f}\".format(f1_decisiontree))\n\n# Confusion matrix visualization\n\ndef confusionM_decisiontree(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_decisiontree(y_test,y_pred,class_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:52.928337Z","iopub.execute_input":"2023-03-22T17:45:52.928814Z","iopub.status.idle":"2023-03-22T17:45:53.271735Z","shell.execute_reply.started":"2023-03-22T17:45:52.928765Z","shell.execute_reply":"2023-03-22T17:45:53.270188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"markdown","source":"A random forest is a type of ensemble learning method in machine learning, and it is based on constructing multiple decision trees and combining their outputs to make a final prediction.","metadata":{}},{"cell_type":"code","source":"# Train the Random Forest classifier\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = randomforest.predict(x_test)\n\n#KFOLD cross validation score\n\nscores = cross_val_score(randomforest, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_randomforest =  round(scores.mean()* 100, 2)\nprint(\"Mean:\", mean_randomforest)\nprint(\"Standard Deviation:\", scores.std())\n\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\ncm_randomforest = confusion_matrix(y_test, y_pred) \nprecision_randomforest = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_randomforest = round(recall_score(y_test, y_pred) * 100, 2)\nf1_randomforest = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\nprint(\"Confusion matrix:\\n\", cm_randomforest)\nprint(\"Accuracy: {:.2f}\".format(acc_randomforest))\nprint(\"Precision: {:.2f}\".format(precision_randomforest))\nprint(\"Recall: {:.2f}\".format(recall_randomforest))\nprint(\"F1 score: {:.2f}\".format(f1_randomforest))\n\n# Confusion matrix visualization\n\ndef confusionM_randomforest(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_randomforest(y_test,y_pred,class_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:53.273394Z","iopub.execute_input":"2023-03-22T17:45:53.273802Z","iopub.status.idle":"2023-03-22T17:45:55.757524Z","shell.execute_reply.started":"2023-03-22T17:45:53.273760Z","shell.execute_reply":"2023-03-22T17:45:55.755068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN or k-Nearest Neighbors","metadata":{}},{"cell_type":"markdown","source":"KNN or k-Nearest Neighbors is a type of supervised machine learning algorithm used for classification and regression tasks. In the KNN algorithm, the class of a new instance is predicted based on the class of its nearest neighbors in the training data.","metadata":{}},{"cell_type":"code","source":"# Train the KNN or k-Nearest Neighbors classifier\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = knn.predict(x_test)\n\n#KFOLD cross validation score\n\nscores = cross_val_score(knn, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_knn =  round(scores.mean()* 100, 2)\nprint(\"Mean:\",mean_knn)\nprint(\"Standard Deviation:\", scores.std())\n\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_knn = round(accuracy_score(y_pred, y_test) * 100, 2)\ncm_knn = confusion_matrix(y_test, y_pred) \nprecision_knn = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_knn = round(recall_score(y_test, y_pred) * 100, 2)\nf1_knn = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\nprint(\"Confusion matrix:\\n\", cm_knn)\nprint(\"Accuracy: {:.2f}\".format(acc_knn))\nprint(\"Precision: {:.2f}\".format(precision_knn))\nprint(\"Recall: {:.2f}\".format(recall_knn))\nprint(\"F1 score: {:.2f}\".format(f1_knn))\n\n# Confusion matrix visualization\n\ndef confusionM_knn(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_knn(y_test,y_pred,class_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:55.760246Z","iopub.execute_input":"2023-03-22T17:45:55.760810Z","iopub.status.idle":"2023-03-22T17:45:56.140373Z","shell.execute_reply.started":"2023-03-22T17:45:55.760752Z","shell.execute_reply":"2023-03-22T17:45:56.139051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stochastic Gradient Descent","metadata":{}},{"cell_type":"markdown","source":"Stochastic Gradient Descent (SGD) is an iterative optimization algorithm used for training machine learning models, particularly for large datasets. It is a variation of the gradient descent algorithm that computes the gradient of the loss function with respect to the model parameters using a random subset of the training data (a.k.a mini-batch), instead of the entire dataset.","metadata":{}},{"cell_type":"code","source":"# Train the Stochastic Gradient Descent classifier\nsgd = SGDClassifier(loss='log', max_iter=1000, random_state=0)\nsgd.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = sgd.predict(x_test)\n\n#KFOLD cross validation score\n\nscores = cross_val_score(sgd, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_sgd = round(scores.mean()* 100, 2)\nprint(\"Mean:\", mean_sgd)\nprint(\"Standard Deviation:\", scores.std())\n\n\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_sgd = round(sgd.score(x_train, y_train) * 100, 2)\ncm_sgd = confusion_matrix(y_test, y_pred) \nprecision_sgd = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_sgd = round(recall_score(y_test, y_pred) * 100, 2)\nf1_sgd = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\nprint(\"Confusion matrix:\\n\", cm_sgd)\nprint(\"Accuracy: {:.2f}\".format(acc_sgd))\nprint(\"Precision: {:.2f}\".format(precision_sgd))\nprint(\"Recall: {:.2f}\".format(recall_sgd))\nprint(\"F1 score: {:.2f}\".format(f1_sgd))\n\n\n# Confusion matrix visualization\n\ndef confusionM_sgd(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_sgd(y_test,y_pred,class_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:56.142111Z","iopub.execute_input":"2023-03-22T17:45:56.142553Z","iopub.status.idle":"2023-03-22T17:45:56.485610Z","shell.execute_reply.started":"2023-03-22T17:45:56.142513Z","shell.execute_reply":"2023-03-22T17:45:56.482508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient Boosting Classifier","metadata":{}},{"cell_type":"markdown","source":"Gradient Boosting Classifier (GBC) is a type of ensemble learning algorithm used for classification problems. It is a variant of the gradient boosting algorithm that combines multiple weak classifiers to form a strong classifier.","metadata":{}},{"cell_type":"code","source":"# Train the Gaussian Naive Bayes classifier\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\n\n# Make predictions on the testing set\ny_pred = gbk.predict(x_test)\n\n#KFOLD cross validation score\n\nscores = cross_val_score(gbk, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nmean_gbk =  round(scores.mean()* 100, 2)\nprint(\"Mean:\", mean_gbk)\nprint(\"Standard Deviation:\", scores.std())\n\n# Calculate the accuracy,confusion matrix,precesion,recall and f1 score of the model\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\ncm_gbk = confusion_matrix(y_test, y_pred) \nprecision_gbk = round(precision_score(y_test, y_pred) * 100, 2)\nrecall_gbk = round(recall_score(y_test, y_pred) * 100, 2)\nf1_gbk = round(f1_score(y_test, y_pred) * 100, 2)\n\n# print the results\nprint(\"Confusion matrix:\\n\", cm_gbk)\nprint(\"Accuracy: {:.2f}\".format(acc_gbk))\nprint(\"Precision: {:.2f}\".format(precision_gbk))\nprint(\"Recall: {:.2f}\".format(recall_gbk))\nprint(\"F1 score: {:.2f}\".format(f1_gbk))\n\n# Confusion matrix visualization\n\ndef confusionM_sgd(y_true,y_predict,target_names):\n    \n#function for confusion matrix visualisation\n\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = train.Survived.unique()\n\nconfusionM_sgd(y_test,y_pred,class_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:56.487522Z","iopub.execute_input":"2023-03-22T17:45:56.490234Z","iopub.status.idle":"2023-03-22T17:45:57.861081Z","shell.execute_reply.started":"2023-03-22T17:45:56.490170Z","shell.execute_reply":"2023-03-22T17:45:57.859749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Model Comparison","metadata":{}},{"cell_type":"markdown","source":"# 9.1 Clustering Models","metadata":{}},{"cell_type":"code","source":"Clustering_models = pd.DataFrame({\n    'Clustering_Models':['Kmeans' ,'Heirarichal','DBSCAN'],\n    'silhouette_score': [silhouette_avg_kmeans,silhouette_avg_h,silhouette_avg_dbscan]})\n    \nClustering_models.sort_values(by='silhouette_score', ascending=False)   ","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:57.862828Z","iopub.execute_input":"2023-03-22T17:45:57.863651Z","iopub.status.idle":"2023-03-22T17:45:57.879099Z","shell.execute_reply.started":"2023-03-22T17:45:57.863594Z","shell.execute_reply":"2023-03-22T17:45:57.877692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9.2 Classification Models","metadata":{}},{"cell_type":"code","source":"models = pd.DataFrame({\n    'Classification_Models': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    \n    'Accuracy Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk],\n     'Precision Score':[precision_svc, precision_knn, precision_logreg, \n              precision_randomforest, precision_gaussian, precision_perceptron,precision_linear_svc, \n                        precision_decisiontree,precision_sgd, precision_gbk],\n      'Recall':[recall_svc, recall_knn, recall_logreg, \n              recall_randomforest, recall_gaussian, recall_perceptron,recall_linear_svc,\n                recall_decisiontree, recall_sgd, recall_gbk],\n       'F1 Score':[f1_svc, f1_knn, f1_logreg, \n              f1_randomforest, f1_gaussian, f1_perceptron,f1_linear_svc, f1_decisiontree,\n              f1_sgd, f1_gbk],})\nmodels.sort_values(by='Accuracy Score', ascending=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:45:57.880939Z","iopub.execute_input":"2023-03-22T17:45:57.881441Z","iopub.status.idle":"2023-03-22T17:45:57.905443Z","shell.execute_reply.started":"2023-03-22T17:45:57.881375Z","shell.execute_reply":"2023-03-22T17:45:57.903855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KFold cross Validation is also performed to check if gradient Boosting itself is best model to be chosen","metadata":{}},{"cell_type":"code","source":"Classification_Models = pd.DataFrame({\n    'Models':['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC',\n              'Decision Tree','Stochastic Gradient Descent', \n              'Gradient Boosting Classifier'],\n    \n   'KFold Score':[mean_svc, mean_knn, mean_logreg,mean_randomforest,\n                  mean_gaussian, mean_perceptron,mean_lsvc, mean_decisiontree,\n                  mean_sgd, mean_gbk],})\n    \nClassification_Models.sort_values(by='KFold Score', ascending=False) ","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:47:00.788583Z","iopub.execute_input":"2023-03-22T17:47:00.789066Z","iopub.status.idle":"2023-03-22T17:47:00.806847Z","shell.execute_reply.started":"2023-03-22T17:47:00.789022Z","shell.execute_reply":"2023-03-22T17:47:00.804609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Model Prediction","metadata":{}},{"cell_type":"markdown","source":"Clustering does not make predictions about the survival of new passengers, as k-means,heirarichal and DBSCAN clustering is an unsupervised learning technique.\n\nBased on the metrics provided in the table, the Gradient Boosting Classifier appears to be the best model for titanic survival data prediction with an accuracy score of 85.79% and a high precision score of 89.47%. \n\nAlso checked KFold Cross Score with 10 folds to make sure the model selection and there as well got Gradient Boosting Classifier with higherst score(80.97)\n\nI decided to use the Gradient Boosting Classifier model for the testing data.","metadata":{}},{"cell_type":"markdown","source":"## 11. Discussion and Conclusion.\n\nFor the titanic survival prediction project, the key findings includes the identification of key features that affect the survival rate, such as gender, age, and class. Various machine learning model, such as a logistic regression or a decision tree etc, can be trained on these features to predict the survival rate with a reasonable accuracy.\n\nThe limitations of the analysis includes the dataset may not be representative of the entire population on board the Titanic, which could limit the generalizability of the findings.\n\nFuture directions for improvement may include incorporating additional features, such as the location of the passenger on the ship or their occupation, or exploring different other machine learning models not used in current project to improve the accuracy of the predictions. Additionally, collecting and incorporating more data, if available, could also help to improve the accuracy of the model.","metadata":{}},{"cell_type":"markdown","source":"# References\n\nWijaya, J. and Agee, J.T., 2014. Machine learning techniques for predictive maintenance of shipboard systems. In Proceedings of the ASNE Ship Maintenance and Modernization Symposium (pp. 1-12).\n\nManikandan, M. and Balamurugan, K., 2015. Predicting survival on the Titanic: A comparison of machine learning techniques. International Journal of Applied Engineering Research, 10(67), pp.49-56.\n\nAlvarez, A., 2017. Exploring the Titanic dataset with R. arXiv preprint arXiv:1703.05921.\n\nNedelcu, O. and Ionescu, A., 2019. Titanic: Machine learning from disaster. In 2019 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR) (pp. 1-5). IEEE.\n\nMishra, S. and Singh, S., 2020. Survival prediction of Titanic passengers using data mining techniques. Journal of Big Data, 7(1), pp.1-15.","metadata":{}}]}